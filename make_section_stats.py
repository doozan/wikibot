#!/usr/bin/python3

import pywikibot
import re
import sys
from collections import defaultdict
from enwiktionary_parser.languages import all_ids as language_constants
from autodooz.sort_sections import sort_prefix, ALL_POS, WT_POS, WT_ELE
from autodooz.sectionparser import SectionParser
import autodooz.fix_section_headers as headerfix

ALL_LANGUAGE_IDS = language_constants.languages
ALL_LANGUAGE_NAMES = { v:k for k,v in ALL_LANGUAGE_IDS.items() }


errors = defaultdict(list)
def log(error, section, notes=None):
    if isinstance(section, str):
        path = section
    elif isinstance(section, SectionParser):
        path = section.title + ":"
    else:
        path = ":".join(reversed(list(section.lineage)))

    print(path, error, notes)
    errors[error].append((path, notes))


error_header = {

    # Errors generated by sectionparser
    "trailing_open_html_comment": "Pages with an unclosed HTML comment",
    "trailing_open_template": "Pages with an unclosed template",
    "trailing_open_nowiki_tag": "Pages with an unclosed  <nowiki><nowiki></nowiki> tag",

    "comment_on_title": "Pages with a comment on the section header line",
    "text_on_title": "Pages with text on the section header line",

    # Errors generated here
    "autofix_title": "Pages with title typos that will be automatically fixed by the bot",
    "autofix_numbered_pos": "Pages with numbered POS entries that will automatically be fixed by the bot",
    "autofix_levels": "Pages with level jumps (eg, L4 section inside a L2 section) that will automatically be normalized by the bot",
    "autofix_empty_section": "Pages with empty sections that will automatically be removed by the bot",

    "first_section_not_l2": "First section on the page is not L2",

    "open_html_comment": "Pages containing a section header inside a HTML comment",
    "open_template": "Pages containing a section header inside a template",
    "open_nowiki_tag": "Pages contaning a section header inside a <nowiki><nowiki></nowiki> tag",

    "unexpected_counter": "Section title has a number but is not a countable section",
    "empty_section": "Pages with an empty section that needs manual review",

    "reference_tag_outside_references": "Pages where <nowiki><references/></nowiki> appears outside of a section named References",
    "ref_tag_without_references": "Pages with a <nowiki><ref></nowiki> tag (or equivalent) but no <nowiki><references/></nowiki> tag",
}

def format_error(error, items):

    if error in error_header:
        yield error_header[error]
        yield "\n"

    yield f"{len(items)} entries"
    yield "\n"

    for path, notes in sorted(items):
        page, extra = path.split(":", 1)
        line = []
        if len(extra) > 1:
            line.append(f": [[{page}|{path}]]")
        else:
            line.append(f": [[{page}]]")
        if notes:
            if "{{" in notes or "<" in notes:
                line.append(f" (<nowiki>{notes}</nowiki>)")
            else:
                line.append(f" ({notes})")

        yield "".join(line)

def export_error(error, items):
    page = "User:JeffDoozan/lists/" + error
    save_page(page, "\n".join(format_error(error, items)))

def export_errors():
    for error, items in errors.items():
        export_error(error, items)

    # Update pages that no longer have any entries
    for error in error_header.keys()-errors.keys():
        export_error(error, [])


def validate_entry(entry):

    for x in entry._log:
        error, path, line = x
        log(error, path, line)

    headerfix.fix_section_titles(entry) and log("autofix_title", entry)
    headerfix.fix_remove_pos_counters(entry) and log("autofix_numbered_pos", entry)
    headerfix.fix_section_levels(entry) and log("autofix_levels", entry)
    headerfix.remove_empty_sections(entry) and log ("autofix_empty_section", entry)

    if not entry._children:
        log("empty_page", entry)
        return

    if entry._children[0].level != 2:
        log("first_section_not_l2", entry._children[0])

    if entry.state & 1:
        log("trailing_open_html_comment", entry)
    if entry.state & 2:
        log("trailing_open_template", entry, entry.template_depth[-1])
    if entry.state & 4:
        log("trailing_open_nowiki_tag", entry)

    for section in entry.ifilter_sections():

        if section.count and not section.title in headerfix.COUNTABLE_SECTIONS:
            log("unexpected_counter", section)

        if not section._lines and not section._children:
            log("empty_section", section)

    PATTERN_SIMPLE_REFS = r"(?i)(<\s*references\s*/>|{{reflist}})"
    refs = entry.ifilter_sections(matches=lambda x: any(d for d in x._lines if re.search(PATTERN_SIMPLE_REFS, d)))
    for r in refs:
        if "References" not in r.lineage:
            log("reference_tag_outside_references", r)

    for section in entry._children:
        if re.search(headerfix.PATTERN_REF_TAGS, str(section)) and not re.search(headerfix.PATTERN_REFS, str(section)):
            note = "has references section" if any(x for x in section._children if x.title == "References") else None
            log("autofix_missing_references", section, note)



SAVE_NOTE = None
def main():

    import argparse
    from pywikibot import xmlreader
    global SAVE_NOTE

    parser = argparse.ArgumentParser(description="Find fixable entries")
    parser.add_argument("xmlfile", help="Wiktionary dump")
    parser.add_argument('--summary', help="Summary comment", required=True)
    parser.add_argument("--upload-stats", help="Update stats on Wiktionary", action='store_true', default=False)
    parser.add_argument("--tag", help="tag to use when uploading data (if specified multiple times will upload to each location)", action='append')
    parser.add_argument('--verbose', action='store_true')
    args = parser.parse_args()

    SAVE_NOTE = args.summary

    if args.upload_stats:
        if not args.tag:
            print("--tag is required when using --upload-stats")
            return 1

    dump = xmlreader.XmlDump(args.xmlfile)
    parser = dump.parse()

    stats = defaultdict(int)
    samples = defaultdict(set)

    count = 0
    for page in parser:
        if ":" in page.title  or "/" in page.title or page.isredirect:
            continue

        count += 1
        if count % 1000 == 0 and args.verbose:
            print(count, file=sys.stderr, end="\r")
#            if count >= 2000:
#                break

        entry = SectionParser(page.text, page.title)

        validate_entry(entry)

        for section in entry.ifilter_sections():
            item = f"{section.level}:{section.title}"
            stats[item] += 1
            if samples[item] is not None:
                samples[item].add(page.title)
                if len(samples[item]) > 100:
                    samples[item] = None

    export_errors()

    if args.upload_stats:
        for tag in args.tag:
            base_url = "User:JeffDoozan/stats/sections/" + tag
            upload_stats(base_url, stats)
            upload_samples(base_url, samples)



def upload_samples(base_url, samples):

    sections = defaultdict(lambda: defaultdict(dict))

    for item, samples in sorted(samples.items()):
        if not samples:
            continue

        level, title = item.split(":",1)
        section = title[0] if title[0].isalnum() else "Other"
        sections[section][title][level] = samples

    for section, titles in sections.items():
        data = []
        data.append("; Sections with <100 uses at a given level")
        data.append(" | ".join(f"[[{base_url}/{x}|{x}]]" if x != section else x for x in sorted(sections.keys(), key=lambda x: x if x == "Others" else "a"+x)))

        for title, levels in sorted(titles.items()):
            if all(x.isalnum() or x.isspace() for x in title):
                data.append(f"==={title}===")
            else:
                data.append(f"===<nowiki>{title}</nowiki>===")
            for level, samples in levels.items():
                data.append(f"; L{level}: " + ", ".join(f"[[{x}#{title}|{x}]]" for x in sorted(samples)))

        page = base_url + f"/{section}"
        page_text = "\n".join(data)
#        print(page_text)
#        return
#
        save_page(page, page_text)

site = None
def save_page(page, page_text):
    global site
    if not site:
        site = pywikibot.Site()
    wiki_page = pywikibot.Page(site, page)
    if wiki_page.text == page_text:
        print(f"{page} has no changes")
        return
    wiki_page.text = page_text
    print(f"saving {page}")
    wiki_page.save(SAVE_NOTE)

def upload_stats(base_url, stats):

    title_stats = defaultdict(dict)
    table_types = {}
    max_level = defaultdict(int)

    for item, count in sorted(stats.items()):
        level, title = item.split(":",1)
        level = int(level)
        title_stats[title][level] = count

        if title not in table_types:
            if title in ALL_LANGUAGE_NAMES:
                table_type = "Languages"
            elif title in WT_POS:
                table_type = "WT:POS"
            elif title in WT_ELE:
                table_type = "WT:ELE"
            else:
                table_type = "Nonstandard"
            table_types[title] = table_type
        else:
            table_type = table_types[title]

        if level > max_level[table_type]:
            max_level[table_type] = level

    tables = defaultdict(list)
    for table_type in ["Languages", "WT:POS", "WT:ELE", "Nonstandard"]:
        tables[table_type] = []
        header = ["Section"]
        for level in range(2,max_level[table_type]+1):
            header.append(f"L{level}")
        header.append("Total")
        tables[table_type].append(header)

    for title in sorted(title_stats):
        table_type = table_types[title]
        table = tables[table_type]
        row = []
        row.append(f"{title}")
        total = 0
        for level in range(2,max_level[table_type]+1):
            count = title_stats[title].get(level, "")
            if count:
                total += count
            row.append(count)
        row.append(total)
        table.append(row)


    res = []
    for table_type in ["Languages", "WT:POS", "WT:ELE", "Nonstandard"]:
        table = tables[table_type]
        totals = [0] * (max_level[table_type])
        for row in table[1:]:
            for i,count in enumerate(row[1:]):
                if count:
                    totals[i] += count
                    # make the title linkable if there's a category with < 100
                    if count < 100 and not row[0].startswith("[["):
                        title = row[0]
                        section = title[0] if title[0].isalnum() else "Other"
                        if all(x.isalnum() or x.isspace() for x in title):
                            row[0] = f"[[{base_url}/{section}#{title}|{title}]]"
                        else:
                            row[0] = f"[[{base_url}/{section}#{title}|<nowiki>{title}</nowiki>]]"

        summary = ["Total"] + totals
        table.append(summary)

        title = f"{table_type} sections"
        res.append(f"""<div class="NavFrame">
<div class="NavHead" style="text-align: left;">{title}</div>
<div class="NavContent derivedterms" style="text-align: left;">""")

        res.append(make_wiki_table(table, extra_class="sortable", num_headers=1, num_footers=1))
        res.append("</div></div>")

    page_text = "\n".join(res)
#    print(page_text)
#    return

    page = base_url
    save_page(page, page_text)


def make_wiki_table(rows, caption=None, extra_class=None, num_headers=0, num_footers=0):
    cls = f"wikitable {extra_class}" if extra_class else "wikitable"
    lines = ['{| class="' + cls + '"' ]
    if caption:
        lines.append(f'|+ class="nowrap" | {caption}')
    for i, row in enumerate(rows):
        lines.append("|-")
        divider = "!" if i<num_footers or i>len(rows)-1-num_footers else "|"
        lines.append(divider + (divider*2).join(map(str,row)))

    lines.append("|}")

    return "\n".join(lines)


if __name__ == "__main__":
    main()
