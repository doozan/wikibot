#!/usr/bin/python3

from collections import defaultdict, namedtuple
from enwiktionary_wordlist.wikiextract import WikiExtract, WikiExtractWithRev
import autodooz.wikimatch as wikimatch
import itertools
import re
import sys

import pywikibot
from pywikibot import pagegenerators, textlib
from pywikibot.bot import (
    AutomaticTWSummaryBot,
    ConfigParserBot,
    ExistingPageBot,
    SingleSiteBot,
)

# This is required for the text that is shown when you run this script
# with the parameter -help.
docuReplacements = {'&params;': pagegenerators.parameterHelp}  # noqa: N816

# A safety check, if this appears in a patch, it will never be written to the live site
FAILSAFE_TEXT="AUTODOOZ"

class Logger:
    def __init__(self, opt):
        self.opt = opt
        self.first_patch = True

        # Only use compact mode when manually specified, user is responsible if it doesn't work
        self.compact = self.opt.compact

        if opt.log_matches:
            self.matches = sys.stdout if not opt.log_matches or opt.log_matches == "-" else open(opt.log_matches, 'w')
        else:
            self.matches = None

        if opt.log_fixes:
             self.fixes = sys.stdout if opt.log_fixes == "-" else open(opt.log_fixes, 'w')
        else:
            self.fixes = None

    def dump_options(self):

        meta_entries = []

        entry = " ".join(sys.argv)
        meta_entries.append(wikimatch.format_entry("00-autodooz-cmdline", entry, self.compact))

#        arguments = vars(self.opt)
#        values = []
#        for k in ["context", "no_children"]:
#            values.append(f"{k}={arguments.get(k)}")
#
#        joiner = "; " if self.compact else "\n"
#        entry = joiner.join(values)
#        if entry:
#            meta_entries.append(wikimatch.format_entry("00-autodooz-forced-options", entry, self.compact))

        for data in meta_entries:
            if self.matches:
                print(data, file=self.matches)
            if self.fixes and self.fixes != self.matches:
                print(data, file=self.fixes)

    def log_patch(self, patch, article):
        if self.first_patch:
            self.dump_options()
            self.first_patch = False

        match, fix = patch

        if self.matches:
            entry = wikimatch.format_match(match, article.text, compact=self.compact)
            if entry:
                print(entry, file=self.matches)

        if self.fixes:
            entry = wikimatch.format_fix(match, fix, article.text, compact=self.compact)
            if entry:
                print(entry, file=self.fixes)

log = None

def load_fixfile(filename):

    with open(filename) as infile:
        first_line = next(infile)
        infile.seek(0)

        if first_line.startswith("_____"):
            return {k:v for k,v in WikiExtract.iter_articles(infile)}

        else:
            res = {}
            count = 0
            for item in infile:
                count += 1
                header, _, line = item.partition(": ")
                if _ != ": ":
                    print(f"Invalid line {count}: {item}", file=sys.stderr)
                    exit(1)
                res[header] = line.rstrip("\n")
            return res

def find_patches(fixes, text, title, path_filter, match_context, no_children, should_match):
    # Should match indicates this is a patch loaded from a file and it's expected to match

    patches = []

    for fix in fixes:
        matches = get_fix_matches(fix, text, title, path_filter, match_context, no_children)
        if not matches:
            if should_match:
                print(f"Failed to prepare patch for '{title}', page has probably changed", file=sys.stderr)
                #raise ValueError(f"Failed to prepare patch for '{title}', page has probably changed")
            continue

        # wildcards and regexes may generate multiple matches
        for match in matches:
            patches.append((match, fix))

    return patches


def make_patches(all_fixes, text, title, path_filter, match_context, no_children, should_match):
    """ Returns a list of (Match, Fix) """

    page_fixes = all_fixes.get(title, []) # page-specific fixes
    if page_fixes:
        return find_patches(fixes, text, title, path_filter, match_context, no_children, should_match=True)

    fixes = all_fixes.get(":fix:")
    if not fixes:
        raise ValueError(f"no fixes available for '{title}', this should never happen")

    pre_patches  = find_patches(all_fixes.get(':pre_fix:',  []), text, title, path_filter, match_context, no_children, should_match)
    text = apply_patches(text, title, pre_patches, [])

    patches      = find_patches(all_fixes.get(':fix:',      []), text, title, path_filter, match_context, no_children, should_match)
    text = apply_patches(text, title, patches, [])

    # Pre and post patches are only applied if there are real patches applied
    if not patches:
        return []
    post_patches = find_patches(all_fixes.get(':post_fix:', []), text, title, path_filter, match_context, no_children, should_match)

    return pre_patches + patches + post_patches


def apply_patches(text, title, patches, summary):

    # Apply patches from end to the beginning so their edits won't affect
    # the offsets of remaining patches

    prev_match = None
    for match, fix in sorted(patches, key=lambda x: x[0].start, reverse=True):
        if prev_match and prev_match.start < match.end:
            # TODO: patches of type 'function' can overlap
            print("patches overlap, let's see what happens!")
#            raise ValueError("Patches overlap", prev_match, match)
        prev_match = match

        new_text = wikimatch.get_fixed_text(match, fix, text, title, summary)

        t = list(text)
        t[match.start:match.end] = new_text
        text = "".join(t)

    return text



def get_fix_matches(fix, text, title, path_filter, match_context, no_children):
    """Returns start, end line numbers affected by the given fix
    Returns None if fix can't be applied unambiguously to the given text
    """

    if fix.type == "function":
        if not callable(fix.old):
            raise ValueError("fix must be callable when using --mode function")

        summary = []
        new_text = fix.old(text, title, summary, fix.new)
        if new_text == text:
            return

        # Generate a full-page match
        return [wikimatch.Match(title, None, None, 0, len(text))]

    if callable(fix.old):
        raise ValueError("search pattern is callable, use --mode function")

    multiline = fix.type != "line"

    if fix.type == "regex":
        re_match = fix.old
    elif fix.type == "text":
        re_match = re.escape(fix.old)
    elif fix.type in ["section", "line"]:
        re_match = r"^" + re.escape(fix.old) + r"$"
    else:
        raise ValueError("unhandled fix.type", fix.type)

    matches = wikimatch.get_matches(
            title,
            text,
            re_match,
            re_not = None,
            match_context=match_context,
            no_path = False,
            path_filter=path_filter,
            no_children=no_children)

    if not matches:
        return

    if fix.type in ["section", "line"] and not validate_matches(text, matches, fix):
        return

    return matches

def validate_matches(text, matches, fix):

    if fix.pos.target_max and (fix.pos.target_max == "*" or fix.pos.target_max > 1):
        if fix.pos.target_max != "*" and fix.pos.target_max != len(matches):
            print("matches {len(matches)} times but expected to match {fix.pos.target_max} times")
            return

        # Trim matches to just the indexed match
        if fix.pos.target_idx != "*":
            matches=[matches[fix.pos.target_idx]]
    else:
        if len(matches) > 1:
            print("unexpectedly matches multiple options", fix)
            return

    if fix.pos.section_max:
        raise ValueError("duplicate section matches, not implemented yet")

    for match in matches:
        # Ensure the match exactly matches the old
        # Mosty a sanity check, but also important when doing section matches
        # to detect if lines have been appended to the section (the new lines
        # will be in the match, but won't be in the fix.old)
        current_text = text[match.start:match.end]
        if current_text != fix.old:
            # Not an exact match, see if it's just changes in blank lines or leading/trailing whitespace
            detected_lines = [l.strip() for l in current_text.splitlines() if l.strip()]
            expected_lines = [l.strip() for l in fix.old.splitlines() if l.strip()]
            if detected_lines != expected_lines:
                print("section doesn't match exactly, not matching")

                print('-'*40, "Detected:")
                print(current_text)
                print('-'*40, "Expected:")
                print(fix.old)
                print('-'*40)
                print(len(detected_lines), len(expected_lines))
                for x, line in enumerate(detected_lines):
                    if line != expected_lines[x]:
                        print([line], [expected_lines[x]])
                    else:
                        print(line)
                return

    return True


def check_balanced(text):
    """Verifies that a given replacement string has an equal number of paired characters """

    for a,b in [("{","}"), ("(",")"), ("[","]"), ("<",">"), ("<--", "-->")]:
        if text.count(a) != text.count(b):
            return f"mismatched {a} and {b}"


Fix = namedtuple("Fix", ["path", "pos", "old", "new", "type"])
def load_fixes(oldFile, newFile):
    """ returns a list of Fix objects """
    errors = set()

    old = load_fixfile(oldFile)
    new = load_fixfile(newFile)

    if len(old) != len(new):
        errors.add("mismatched_fixes")
        print(f"Mismatched fixes: old has {len(old)}, new has {len(new)}")

    all_fixes = defaultdict(list)

    first = True
    indexed = defaultdict(lambda: defaultdict(list))
    for title, entry in new.items():
        if first:
            # Leading entries that begin "00-" are meta entries
            # that may contain skip items that start with "00-"
            if title.startswith("00-"):
                # TODO: handle the entry
                continue

            first = False

        # TODO: support for "range" fixes?
        fix_type = "line" if "\n" not in entry else "section"

        # New file references a page that wasn't in the oldfile
        if title not in old:
            errors.add("new_page")
            print(f"{title} exists in {newFile} but not in {oldFile}")
            continue

        # No changes between the two targets
        if old[title] == entry:
            continue

        page,*path,position = title.split(":")
        if path:
            path = tuple(path)

        if entry.strip() == "":
            print(f"ERROR: {title}: replacement text is empty", file=sys.stderr)
            # TODO: don't continue
            continue
            errors.add("blank_replacement")

        error = check_balanced(entry)
        if error:
            errors.add("bad_replacement")
            print(f"ERROR: {title}: {error}", file=sys.stderr)

        if FAILSAFE_TEXT in entry:
            errors.add("fix_has_failsafe")
            print(f"ERROR: {title}: fix contains failsafe text '{FAILSAFE_TEXT}': '{entry}' ", file=sys.stderr)

        pos = parse_position(position)
        fix = Fix(path, pos, old[title], entry, fix_type)

        if pos.section_max:
            raise ValueError("indexed sections not yet supported")

        # indexed patches are grouped and buffered so they can be validated
        if pos and pos.target_max:
            indexed[page][path].append(fix)
        else:
            all_fixes[page].append(fix)


    # Validate indexed patches
    for page, sections in indexed.items():
        for section, fixes in sections.items():

            unique_max = list({f.pos.target_max for f in fixes})
            if len(unique_max) != 1:
                errors.add("multi_target_max")
                print(f"ERROR: {page} {section}: has differing max values {[f.pos for f in fixes]}", file=sys.stderr)
                continue

            # fail on duplicate index keys
            unique_idx = list({f.pos.target_idx for f in fixes})
            if len(unique_idx) != len(fixes):
                errors.add("dup_index")
                print(f"ERROR: {page} {section}: has duplicate index values {[f.pos for f in fixes]}", file=sys.stderr)
                continue

            if "*" in unique_idx:
                if len(unique_idx) != 1:
                    errors.add("mixed_wildcard")
                    print(f"ERROR: {page} {section}: has wildcard with other index values {[f.pos for f in fixes]}", file=sys.stderr)
                    continue

            elif len(fixes) != unique_max[0]:
                print(f"WARNING: {page} {section}: is missing an indexed item seen: {sorted(unique_indices)}", file=sys.stderr)

            all_fixes[page] += fixes

    if not len(all_fixes):
        errors.add("no_fixes")
        print(f"Mismatched fixes: old has {len(old)}, new has {len(new)}, 0 in common")

    if errors:
        # TODO: check if it's allowed to run with errors
        raise ValueError("fix files have errors, not running")

    return all_fixes


Position = namedtuple("Position", ["section_idx", "section_max", "target_idx", "target_max"])
def parse_position(position):
    section_idx = None
    section_max = None
    target_idx = None
    target_max = None

    # check for {x,y} sections index
    match = re.match(r"{(.*?)}", position)
    if match:
        items = list(map(str.strip, match.group(1).split(",")))
        if len(items) != 2 or any(not x.isnumeric() for x in items):
            raise ValueError(f"invalid section index: {match.group(0)} (expected: {{x,y}})")
        section_start, section_end = map(int,items)
        if section_start > section_end:
            raise ValueError(f"invalid section index: {match.group(0)} pos>idx")

    # check for [x,y] match index
    match = re.match(r"\[(.*?)\]", position)
    if match:
        items = list(map(str.strip, match.group(1).split(",")))
        if len(items) == 1 and items[0] == "*":
            target_idx = target_max = "*"
        elif len(items) != 2:
            raise ValueError(f"invalid section index: {match.group(0)} (expected: [x,y])")
        else:
            for item in items:
                if not (item == "*" or item.isnumeric()):
                    raise ValueError(f"invalid section index: {match.group(0)} (expected: [x,y])")
            target_idx = int(items[0]) if items[0].isnumeric() else items[0]
            target_max = int(items[1]) if items[1].isnumeric() else items[1]
            if target_idx.isnumeric() and target_max.isnumeric() and target_idx > target_max:
                raise ValueError(f"invalid section index: {match.group(0)} pos>idx")
        if target_max == "*" and target_idx != "*":
            raise ValueError(f"invalid section index: {match.group(0)}, cannot mix index with undefined limit")


    return Position(section_idx, section_max, target_idx, target_max)


class WikiExtractReplacePageGenerator:

    """
    Iterator that will yield Pages that might contain text to replace.

    These pages will be retrieved from a local wixi extract file.

    :param filename: The dump's path, either absolute or relative
    :type filename: str
    :param start: Skip all articles in the dump before this one
    :type start: str
    :param fixes: A list of Fix objects
    :type fixes: list of Fix objects
    :param exceptions: A dictionary which defines when to ignore an
        occurrence. See docu of the ReplaceRobot initializer below.
    :type exceptions: dict
    """

    def __init__(self, filename, start, fixes, exceptions, site, opt):

        """Initializer."""
        self.filename = filename
        self.exceptions = exceptions
        self.start = start
        self.skipping = bool(start)
        self.fixes = fixes
        self.opt = opt
        self.count = 0

#        self.replacements = []
#        for fix in fixes:
#            self.replacements.append(Replacement(fix.old, fix.new))

#        for replacement in self.replacements:
#            #replacement.compile(regex, flags)
#            replacement.compile(True, 0)

        self.excsInside = []
        if 'inside-tags' in self.exceptions:
            self.excsInside += self.exceptions['inside-tags']
        if 'inside' in self.exceptions:
            self.excsInside += self.exceptions['inside']

        if site:
            self.site = site
        else:
            self.site = pywikibot.Site()

        if filename.endswith(".xml.bz2"):
            dump = pywikibot.xmlreader.XmlDump(filename)
            self.parser = dump.parse()

        elif filename.endswith(".txt.bz2"):
            from enwiktionary_wordlist.wikiextract import WikiExtractWithRev
            self.parser = WikiExtractWithRev.iter_articles_from_bz2(filename)

        else:
            print("unrecognized file type", filename)
            exit(1)


    def __iter__(self):
        """Iterator method."""
        try:
            for article in self.parser:
                if self.skipping:
                    if article.title != self.start:
                        continue
                    self.skipping = False
                if self.isTitleExcepted(article.title) \
                        or self.isTextExcepted(article.text):
                    continue

                patches = make_patches(self.fixes, article.text, article.title, self.opt.path_filter, self.opt.context, self.opt.no_children, False)

                if not patches:
                    continue

                # TODO: Logging multiple patches may need special handling log_patches()?
                for patch in patches:
                    log.log_patch(patch, article)

                if self.opt.read_only:
                    continue

                yield pywikibot.Page(self.site, article.title)

                self.count += 1
                if self.opt.limit and self.count >= self.opt.limit:
                    print("limit reached, exiting", file=sys.stderr)
                    break


        except KeyboardInterrupt:
            with suppress(NameError):
                if not self.skipping:
                    pywikibot.output(
                        'To resume, use "--start:{}" on the command line.'
                        .format(entry.title))

    def isTitleExcepted(self, title):
        """
        Return True if one of the exceptions applies for the given title.

        :rtype: bool
        """
        if ":" in title:
            return True
        if 'title' in self.exceptions:
            for exc in self.exceptions['title']:
                if exc.search(title):
                    return True
        if 'require-title' in self.exceptions:
            for req in self.exceptions['require-title']:
                if not req.search(title):  # if not all requirements are met:
                    return True

        return False

    def isTextExcepted(self, text):
        """
        Return True if one of the exceptions applies for the given text.

        :rtype: bool
        """
        if 'text-contains' in self.exceptions:
            for exc in self.exceptions['text-contains']:
                if exc.search(text):
                    return True
        return False

class ListPageGenerator:
    def __init__(self, pages, start, site):
        self.site = site
        self.pages = pages
        self.start = start
        self.skipping = bool(start)

    def __iter__(self):
        for title in self.pages:
            if self.skipping:
                if article.title != self.start:
                    continue
                self.skipping = False

            yield pywikibot.Page(self.site, title)


class AutoDooz(
    # Refer pywikobot.bot for generic bot classes
    SingleSiteBot,  # A bot only working on one site
    ConfigParserBot,  # A bot which reads options from scripts.ini setting file
    ExistingPageBot,  # CurrentPageBot which only treats existing pages
):

    def __init__(self, generator, fixes, **kwargs):
        """Initializer."""
        self.available_options.update({
            'read_only': None,
            'summary': None,
            'path_filter': None,
            'limit': None,
            'context': None,
            'no_children': False,
        })
        super().__init__(generator=generator, **kwargs)
        self.fixes = fixes
        self.count = 0


    def current_page(self, page: 'pywikibot.page.BasePage') -> None:
        # This never seems to get called, but ommitting it results in current_page() being called in
        # pywikibot/bot.py which emits unwanted info on stdout when piping to files
        raise ValueError("XX")


    def apply_patches(self, text, patches, summary):
        return apply_patches(text, self.current_page.title(), patches, summary)

    def treat_page(self) -> None:
        original_text = self.current_page.text
        title = self.current_page.title()

        if ":" in title:
            return

        patches = make_patches(self.fixes, original_text, title, self.opt.path_filter, self.opt.context, self.opt.no_children, should_match=False)
        if not patches:
            return

        for patch in patches:
            log.log_patch(patch, self.current_page)

        if self.opt.read_only:
            return

        alt_summary = []
        new_text = self.apply_patches(original_text, patches, alt_summary)
        if not new_text:
            return

        summary = "; ".join(alt_summary) if alt_summary else self.opt.summary

        if FAILSAFE_TEXT in new_text:
            raise ValueError(title, "Failsafe detected in text about to be written to site, failing")

        context = 10
        while True:

            # Show the title of the page we're working on.
            # Highlight the title in purple.
            print(">"*10, self.current_page.title(), "<"*10)

            pywikibot.showDiff(original_text, new_text, context=context)
            if self.opt.always:
                break

            print(f"> Summary: {summary}")
            choice = pywikibot.input_choice(
                'Do you want to accept these changes?',
                [('Yes', 'y'), ('No', 'n'), ('Edit original', 'e'),
                 ('edit Latest', 'l'), ('open in Browser', 'b'),
                 ('More context', 'm'), ('All', 'a')],
                default='N')
            if choice == 'm':
                context = context * 3 if context else 3
                continue
            if choice in ('e', 'l'):
                text_editor = editor.TextEditor()
                edit_text = original_text if choice == 'e' else new_text
                as_edited = text_editor.edit(edit_text)
                # if user didn't press Cancel
                if as_edited and as_edited != new_text:
                    new_text = as_edited
                    if choice == 'l':
                        # prevent changes from being applied again
                        last_text = new_text
                continue
#            if choice == 'b':
#                pywikibot.bot.open_webbrowser(page)
#                try:
#                    original_text = page.get(get_redirect=True, force=True)
#                except NoPageError:
#                    pywikibot.output('Page {} has been deleted.'
#                                     .format(page.title()))
#                    break
#                new_text = original_text
#                last_text = None
#                continue
            if choice == 'a':
                self.opt.always = True
            if choice == 'y':
                self.save(self.current_page, original_text, new_text, summary,
                          show_diff=False, asynchronous=True, minor=False)

            # choice must be 'N'
            break

        if self.opt.always and new_text != original_text:
            self.save(self.current_page, original_text, new_text, summary,
                      show_diff=False, asynchronous=False, minor=False)

        self.count += 1
        if self.opt.limit and self.count >= self.opt.limit:
            print("fix limit reached, exiting", file=sys.stderr)
            exit()

    def save(self, page, oldtext, newtext, summary, **kwargs):
        """Save the given page."""

        if not summary:
            print(f"No summary provided when applying fix to '{page}'", file=sys.stderr)
            print("use --summary, or ensure fixes generate a summary", file=sys.stderr)
            exit(1)

        self.userPut(page, oldtext, newtext,
                     summary=summary,
                     ignore_save_related_errors=True, **kwargs)

    def user_confirm(self, question):
        """Always return True due to our own input choice."""
        return True

class Config:
    def __init__(self, filename):
        """Load the fixes from the given filename."""
        # load binary, to let compile decode it according to the file header
        with open(filename, 'rb') as f:
            exec(compile(f.read(), filename, 'exec'), globals())

        self.wikifix = wikifix

def init_fixes(requested_fixes, config):

    pre_fixes = []
    fixes = []
    post_fixes = []

    for fix in requested_fixes:
        try:
            fix = config.wikifix[fix]
        except KeyError:
            pywikibot.output('Available predefined fixes are: {}'
                             .format(', '.join(config.wikifix.keys())))
            exit(1)

        if not fix['fixes']:
            pywikibot.warning('No fixes defined for fix "{}"'
                              .format(fix_name))
            exit(1)

#        # override command line options with fix values? (maybe better the other way around?)
#        for key in ["mode", "context", "path_filter", "summary"]:
#            if key in fix:
#                setattr(args, key, fix[key])

        for search, replacement in fix.get('pre-fixes', []):
            pre_fixes.append(Fix(None, None, search, replacement, fix['mode']))

        for search, replacement in fix['fixes']:
            fixes.append(Fix(None, None, search, replacement, fix['mode']))

        for search, replacement in fix.get('post-fixes', []):
            post_fixes.append(Fix(None, None, search, replacement, fix['mode']))

    return pre_fixes, fixes, post_fixes

def main():

    local_args = pywikibot.handle_args()
    gen_factory = pagegenerators.GeneratorFactory()
    local_args = gen_factory.handle_args(local_args)

    import argparse

    parser = argparse.ArgumentParser(description="Search language extract for articles matching a given pattern")
    parser.add_argument("--config", help="Path to autodooz-fixes config file")
    parser.add_argument("--load-matches", help="Load per-page match data from specified file")
    parser.add_argument("--load-fixes", help="Load per-page replacement data from specified file (requires --load-matches)")
    parser.add_argument("--path-filter", help="Search only inside pages/sections('page:section:subsection:...') that match the given regex")
    parser.add_argument("--file", help="Read pages from a wiki dump or language extract file")
    parser.add_argument("--start", help="Resume from a given page in the file")

    parser.add_argument("--read-only", help="Only log matches/replacements, don't write any changes to wiki", action='store_true')
    parser.add_argument("--log-matches", help="Write matches to the given file (use - for stdout)")
    parser.add_argument("--log-fixes", help="Write replacements to the given file (use - for stdout)")

    parser.add_argument("--pre-fix", help="Run a fix before running other fixes, changes are applied only if another fix is applied", action='append')
    parser.add_argument("--fix", help="Run a fix declared in autodooz-fixes")
    parser.add_argument("--post-fix", help="Run a given fix only if an earlier fix has been applied", action='append')

    parser.add_argument("--summary", help="Summary message to use when saving wiki page")

    parser.add_argument("--search", help="Only search for the given pattern, make no replacements (implies --read-only and --log-matches -)", action='append')

    parser.add_argument("--compact", help="Log matches in compact mode", action='store_true')
    parser.add_argument("--regex", help="Search/replace with regex patterns", action='store_true')
    parser.add_argument("--context", help="How much surrounding data in included with the match", choices=["line", "section", "none"], default="line")
    parser.add_argument("--limit", type=int, help="Stop after N pages have been fixed")

    parser.add_argument("--no-children", help="Don't include child sections of matching sections", action="store_true")
    parser.add_argument("--always", help="Apply fixes without user-input", action="store_true")

    parser.add_argument("match", nargs='*')

    args = parser.parse_args(local_args)

    site = pywikibot.Site()

    gen = None

    fixes = {}
    pre_fixes = None
    post_fixes = None

    # Load user fixes first because it can modify the args
    if args.fix:

        from pathlib import Path
        filename = args.config if args.config else "~/.pywikibot/autodooz-fixes.py"
        filename = Path(filename).expanduser()
        config = Config(filename)

        pre_fixes, user_fixes, post_fixes = init_fixes([args.fix], config)
        if pre_fixes:
            fixes[":pre_fix:"] = pre_fixes
        fixes[":fix:"] = user_fixes
        if post_fixes:
            fixes[":post_fix:"] = post_fixes

        if args.pre_fix:
            if ":pre_fix:" in fixes:
                print("can't mix --pre-fix with a --fix that defines pre-fixes in its config")
                exit(1)
            _, fixes[":pre_fix:"], _ = init_fixes(args.pre_fix, config)

        if args.post_fix:
            if ":post_fix:" in fixes:
                print("can't mix --post-fix with a --fix that defines post-fixes in its config")
                exit(1)
            _, fixes[":post_fix:"], _ = init_fixes(args.post_fix, config)

    # Setup logging as early as possible

    if args.search:
        args.read_only = True
        if not args.log_matches:
            args.log_matches = "-"

    global log


    log = Logger(args)

    if args.load_fixes and not args.load_matches:
        print("--load-fixes requires --load-matches")
        exit(1)

    if args.search and args.log_fixes:
        print("Running in read-only mode, data for --log-fixes will not be generated")
        exit(1)

#    if args.read_only and args.summary:
#        print("Running in read-only mode, --summary will not be used because no changes will be saved")
#        exit(1)

    if args.read_only and not (args.log_matches or args.log_fixes):
        print("--read-only requires at least on of --log-matches or --log-fixes")
        exit(1)

    if args.fix and args.load_matches:
        print("--fix and --load-matches can't be combined")
        exit(1)

    if (args.fix or args.load_fixes) and args.match:
        print("--fix can't be mixed with command-line search/replace options")
        exit(1)

    if args.load_matches and args.file:
        print("--load-matches can't be used with --file")
        exit(1)

    if args.load_matches and args.fix:
        print("--load-matches can't be used with --fix")
        exit(1)

    if gen and args.load_matches:
        print("--load-matches can't be used with pwb generators (-search, etc)")
        exit(1)

    if args.load_matches:
        fixes = load_fixes(args.load_matches, args.load_fixes)
        gen = ListPageGenerator(args.start, fixes.keys(), site)

    if not fixes and not args.fix:
        generated_fixes = []

        if args.regex:
            mode = "regex"
        else:
            mode = "text"

        if args.search:
            if args.match:
                print("--search cannot be used with additional MATCHES values")
                exit(1)

            replacement = FAILSAFE_TEXT
            for search in args.search:
                if not args.regex and "*" in search:
                    print(f"Warning: search term '{search}' contains wildcard, but --regex not supplied, matching as literal string", file=sys.stderr)
                generated_fixes.append(Fix(None, None, search, replacement, args.mode))

        else:
            if len(args.match)%2 != 0:
                print(f"unmatched search/value pairs, {len(args.match)} values supplied")
                exit(1)

            for i in range(0,len(args.match),2):
                search = args.match[i]
                replacement = args.match[i+1]
                generated_fixes.append(Fix(None, None, search, replacement, args.mode))

        if fixes:
            # Probably fine, but no reason to make things more complicated
            print("mixed user specified and loaded, not supported")
            exit(1)

        fixes[':fix:'] = generated_fixes

    if args.file:
#        if not args.read_only and not args.path_filter:
#            print("--path-filter must be specified when making changes using wikiextracts (recommended --path-filter '.*?:Language:')")
#            exit(1)

        gen = WikiExtractReplacePageGenerator(args.file, args.start, fixes, {}, site, args)

    gen = gen_factory.getCombinedGenerator(gen, preload=True)
    if gen:

        # This is not fast because it requires every page to be processed
        # and returned from the generater before being discarded. However,
        # it works without having to modify the existing pywiki generators
        # and even a slow implementation is better than no implementation
        if args.start:
            for page in gen:
                if str(page.title()) == args.start:
                    gen = itertools.chain([page], gen)
                    break

        bot = AutoDooz(generator=gen, fixes=fixes, **vars(args))
        bot.run()
    else:
        pywikibot.bot.suggest_help(missing_generator=True)

if __name__ == '__main__':
    main()
